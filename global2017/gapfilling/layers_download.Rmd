---
title: "Downloading gapfilling data layers from OHI 2017 analysis"
author: "Mel"
date: "4/27/2018"
output: html_document
---

Based on the paths in "eez_layers_meta_data/layers_eez_gapfill.csv", gapfilling datasets are hunted down and placed in the ohi-global/global2017/gapfill/layers folder.  Gapfilling data includes a _gf extension and is typically located in the same location as the actual data.

This code should be run as a first step if there were changes in the gapfilling data.

```{r layer download, warning=FALSE, message=FALSE}

# load libraries
library(tidyr)
library(dplyr)

setwd('global2017/gapfilling') # comment this out when knitting!!!

# download gap-filling layers
source('https://raw.githubusercontent.com/OHI-Science/ohiprep_v2017/master/src/R/common.R')

#file sources
ohiprep <- "https://raw.githubusercontent.com/OHI-Science/ohiprep_v2017/master/"
ohiglobal <- "https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/"

### directory and file names
gf_file_loc <- read.csv("https://raw.githubusercontent.com/OHI-Science/ohi-global/draft/eez_layers_meta_data/layers_eez_gapfill.csv") %>%
  filter(variable == "file_loc_2017") %>%
  filter(!is.na(variable_data)) %>%
  mutate(variable_data = gsub("ohiprep:", ohiprep, variable_data)) %>%
  mutate(variable_data = gsub("ohi-global:", ohiglobal, variable_data))

for(i in 1:length(gf_file_loc$variable_data)){ # i=1
f_name <- gf_file_loc$layer[i]
 tmp <- read.csv(gf_file_loc$variable_data[i])
 
 write.csv(tmp, sprintf("layers/%s.csv", f_name))
  }


 scenarioList = list(
   eez2015     = list(
    layer   = 'layers_eez',
    fld_dir      = 'dir_gap_fill_2015',
    fld_fn       = 'fn_gap_fill_2015',
    save_dir     = 'global2015'))
  
  
  for (i in length(scenarioList)){ #i=1
  scenario   = names(scenarioList)[[i]]
  fld_dir    = scenarioList[[i]][['fld_dir']]
  fld_fn     = scenarioList[[i]][['fld_fn']]
  layer = scenarioList[[i]][['layer']]
  save_dir = scenarioList[[i]][['save_dir']]

  ## Read in the layers.csv file with paths to the data files
  ## reading archived 2015 file
 g <- read.csv(sprintf("%s.csv", layer), stringsAsFactors = FALSE, na.strings='')    
 
    # replaces 'ohiprep' and 'neptune_data' parts of the filepath with the full file paths
    # 'ohiprep' files are located here: https://github.com/OHI-Science/ohiprep
    # 'neptune_data' files are located on the NCEAS Neptune server
    g$dir_in = sapply(
      str_split(g[[fld_dir]], ':'),   
      function(x){ sprintf('%s/%s', dirs[x[1]], x[2])})
    
    g$fn_in = g[[fld_fn]]
    
    # filters the data and determines whether the file is available, saves a copy to tmp folder
    lyrs = g %>%
      filter(ingest==T) %>%
      mutate(
        path_in        = file.path(dir_in, fn_in),
        path_in_exists = file.exists(path_in),
        filename = sprintf('%s.csv', layer),
        path_out = sprintf('layers/%s', filename)) %>%
      select(
        targets, layer, name, description, 
        fld_value, units,
        path_in, path_in_exists, filename, path_out) %>%
      arrange(targets, layer) %>%
      filter(path_in != "NULL/NA/NA")
    
    # checks that all data layers are available based on file paths 
    if (nrow(filter(lyrs, !path_in_exists)) != 0){
      message('The following layers paths do not exist:\n')
      print(filter(lyrs, !path_in_exists) %>% select(layer, path_in), row.names=F)
      stop('Data cannot be found - check file paths/names in layers.csv' )
    }
    
    # copy layers into specific scenario / layers file 
    for (j in 1:nrow(lyrs)){ # j=4
      stopifnot(file.copy(lyrs$path_in[j], lyrs$path_out[j], overwrite=T))
    }
    
    # delete extraneous files
    files_extra = setdiff(list.files(sprintf('%s/gapFilling/layers', save_dir)), as.character(lyrs$filename))
    unlink(sprintf('%s/gapFilling/layers', save_dir, files_extra))
    
    # layers registry (don't think there is a need for this here, but keeping code in case)
    # write.csv(select(lyrs, -path_in, -path_in_exists, -path_out), 'layers.csv', row.names=F, na='')

}

```

